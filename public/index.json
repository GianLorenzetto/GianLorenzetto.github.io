[{"content":" Background Education Skills Publications Background # I have over 20 years experience as a Senior Developer, a Product Lead, Principal Consultant and now a Senior Engineering Manager at Octopus Deploy. I have successfully delivered unique software solutions over a wide variety of domains, from medical device manufacturing and medical imaging, to mining, finance, IoT, and autonomous vehicle control.\nI have worked with most major platforms and a large array of technologies, from simple websites, to enterprise scale mobile applications and advanced desktop visualization software.\nProfessional Life # In my current role as a Senior Engineering Manager, I am focused on uplifting our internal teams and creating an environment within which our people and teams thrive.\nI\u0026rsquo;m passionate about bringing your whole self to work and as a manager and leader, I am always looking for opportunities to positively challenge and grow the amazing people that I get to work with.\nI firmly believe that in order to build complex software solutions that delight users, teams must be highly collaborative, product led and cross-functional. This is the best way to unearth innovative, but inherently valuable and human-centred software.\nEducation # Ph.D. (Comp Sci)\nAn Investigation Into Trapezoidation and its Application to Geographic Information Systems and Computer Graphics\n2003 The University of Western Australia\nBCM (Majoring in Comp Sci and IT, HONS 1)\nA New Approach to Measuring Image Quality via Frequency Analysis\n1999 The University of Western Australia\nSkills # Below are some of the various skills I\u0026rsquo;ve accumulated over the years, including languages, platforms, paradigms and tooling, in no particular order. Everything listed here I have used to varying degrees, although the majority in a commercial capacity at one time or another (ie, relating to code deployed into a production environment), with a few exceptions, like Rust, where I\u0026rsquo;ve only explored it for professional development or personal hobby projects.\nProgramming Languages # C#, Dart, JavaScript, Rust, C++, C, Ruby, Java, Go, Python, Bash, Lisp, ProLog, Gopher\nDomains # Mobile Applications (Flutter, Xamarin, Swift, Ionic, React Native) Web (React), Web RTC (SignalR), API, REST, GraphQL, Hypermedia (HATEOS) Volumetric (3D) Visualization (medical imaging, OpenGL) Image Processing (DICOM, medical imaging) Rapid Prototyping (STL file processing) Mining (resource estimation, statistical data analysis) Finance (EJB, Cobol, XML/XSLT) Paradigms, Methodologies and Practices # Design Patterns, Design Thinking, Systems Thinking, TDD, DDD, BDD, agile, lean, XP\nOperating Systems # macOS, Windows, Linux, Irix, BeOS, OS/2\nCloud Providers # Azure, AWS, GCP, RackSpace\nPlatforms \u0026amp; Toolchains # ItelliJ based IDEs, VisualStudio, VSCode, Xamarin, WebSphere, XCode, g++, clang, Make, CMake, YML, XML, XAML Git, SVN, CVS, VSS Azure Pipelines, Octopus Deploy, TeamCity, Jenkins, AWS CodeBuild Publications # 2004 # Lorenzetto, G. P. A. \u0026amp; Datta, A. An almost linear-time algorithm for trapezoidation of GIS polygons Future Generation Computer Systems. 20, 7, p. 1145-1155 2004\n2002 # Lorenzetto, G. P. A., Datta, A. \u0026amp; Thomas, R. A fast trapezoidation technique for planar polygons Computers \u0026amp; Graphics. 26, 2, p. 281-289 2002\nLorenzetto, G. P. A. \u0026amp; Datta, A. A Linear Time Heuristics for Trapezoidation of GIS Polygons Lecture Notes in Computer Science. 2331, p. 75-84 2002\n","date":"15 October 2022","permalink":"/pages/about/","section":"Pages","summary":"Background Education Skills Publications Background # I have over 20 years experience as a Senior Developer, a Product Lead, Principal Consultant and now a Senior Engineering Manager at Octopus Deploy.","title":"About Me"},{"content":"","date":"15 October 2022","permalink":"/pages/","section":"Pages","summary":"","title":"Pages"},{"content":" Note that I\u0026rsquo;ve moved my blog! Please visit my Medium profile to see my latest posts. Thanks for reading! ‚ù§Ô∏è ","date":"10 September 2021","permalink":"/posts/","section":"","summary":"Note that I\u0026rsquo;ve moved my blog!","title":""},{"content":"Hello there! üëã # I\u0026rsquo;m a Senior Engineering Manager with Octopus Deploy where I\u0026rsquo;m focused on helping our core platform team deliver amazing outcomes for our Octopus Deploy customers.\nI love technology and I love learning - I\u0026rsquo;m always on the lookout for new languages, ideas, processes and practices to explore and add to my toolbox, to then help the the teams and amazing developers I work with everyday ‚ù§Ô∏è\nOver many years (I\u0026rsquo;ve stopped counting üòÑ) I\u0026rsquo;ve worked on a wide variety of applications and on an even wider variety of operating systems, cloud providers and programming languages.\nüìî Some more about me \u0026hellip;\n","date":"10 September 2021","permalink":"/","section":"Dr Gian Lorenzetto","summary":"Hello there!","title":"Dr Gian Lorenzetto"},{"content":"","date":"14 March 2017","permalink":"/tags/agile/","section":"Tags","summary":"","title":"agile"},{"content":"","date":"14 March 2017","permalink":"/tags/development/","section":"Tags","summary":"","title":"development"},{"content":"","date":"14 March 2017","permalink":"/tags/process/","section":"Tags","summary":"","title":"process"},{"content":"","date":"14 March 2017","permalink":"/tags/scrum/","section":"Tags","summary":"","title":"scrum"},{"content":"","date":"14 March 2017","permalink":"/tags/software/","section":"Tags","summary":"","title":"software"},{"content":"","date":"14 March 2017","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"One problem I\u0026rsquo;ve seen on several occasions now is that of Zombie Scrum, or Scrum-by-numbers. It\u0026rsquo;s an issue whereby someone has implemented Scrum some time back, and the implementation has stagnated.\nSymptoms usually include the team doing all the ceremonies correctly, but not yielding the on-going benefits. A lot of practices (especially the daily Scrum) start to feel forced, or lose purpose.\n(Note, this isn\u0026rsquo;t limited to Scrum, but Scrum is the one I\u0026rsquo;m most familiar with. It can happen with any process, and seems to only get worse the more prescriptive the process is, presumably because buy-in and engagement by the development team was less to begin with.)\nIt\u0026rsquo;s usually categorised by a conversation that goes:\nMe: \u0026ldquo;Do you use an agile process?\u0026rdquo;\nThem: \u0026ldquo;Yes, we do Scrum\u0026rdquo;\nMe: \u0026ldquo;So what\u0026rsquo;s your process look like?\u0026rdquo;\nThem: \u0026ldquo;Err, we do Scrum?\u0026rdquo;\nMe: \u0026ldquo;Ok, but what are your practices like day to day?\u0026rdquo;\nThem: \u0026ldquo;Oh, yeah, we do stand-ups too!\u0026rdquo;\nWhen people transition to Scrum, from something like waterfall, there are a lot of benefits you get immediately: iterative development, tighter feedback loops, increased communication, increased transparency. These are all good things. But old habits die hard, and after a time, it can become very mechanical, and you see the team stabilise and start to cruise again. There is no more improvement in the system. And of course they\u0026rsquo;re doing retro\u0026rsquo;s, right? Probably, but I have rarely seen real, measurable change come out of a retro. Not that retro\u0026rsquo;s are bad, just that they are hard, and you need to be able to quantify whatever it is you\u0026rsquo;re looking to improve to know you\u0026rsquo;ve improved it, and most teams aren\u0026rsquo;t that advanced in their metrics (other than gross measures like a shift in velocity).\nI can see two reasons this happens. The first is that people have been told that if you do Scrum then you have to do it \u0026ldquo;properly\u0026rdquo;. No Scrum-but, or Scrum-hybrid etc. I think this is useful for teams starting out. I think this is detrimental to more advanced teams, teams that have been working together for some time and who have a good understanding of what agile software development is all about (more on this in a bit).\nThe other reason I think this happens is that three ideas have been conflated together: processes and frameworks, practices and techniques and the manifesto for agile software development.\nThe first category, processes and frameworks, are things like Scrum, which mandate certain activities and workflows. The second, practices and techniques, includes things like XP, refactoring, CI etc.\nBy conflating these ideas, usually under the banner capital-A Agile, it\u0026rsquo;s easy for a team to think they are going along great, being agile, when they are missing out on the big payoff: continuous improvement. Yes, they may be doing retro\u0026rsquo;s. But if you consider Agile = Scrum, then you aren\u0026rsquo;t going to see bigger gains after the initial shift. Examples here might include things like: trying mobbing, no estimates, speed pairing, variable length Sprints, and so on.\nI started thinking about this after listening to an Agile For Humans podcast (great podcast), where Ron Quartel discussed how his team was using Scrum, but with no estimates and variable length iterations. As a team, it was working really well for them (and the idea of MVI or Minimum Viable Increment is gold), but the more interesting point is that they where experimenting with their process. In fact, he made reference to discovering the opening statement of the manifesto:\nWe are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: \u0026hellip;\nNow, I\u0026rsquo;m certain I\u0026rsquo;ve read that statement previously, but I too had never fully grasped the importance of the word uncovering. We should be constantly working towards new and better ways of working. Scrum is great, but so is Lean. You like no estimates? Do it. Just remember that agile is an empirical process (more below) you need to measure the outcomes and continuously work to improve.\nLet\u0026rsquo;s look at a couple of the 12 principals from the manifesto (the full list is here):\nOur highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nBusiness people and developers must work together daily throughout the project.\nBuild projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.\nWorking software is the primary measure of progress.\nSimplicity\u0026ndash;the art of maximizing the amount of work not done\u0026ndash;is essential.\nThe best architectures, requirements, and designs emerge from self-organizing teams.\nYou can see that following these principals is key to being agile and much more important than following a prescribed process. In fact, it says this right in the first value of the manifesto:\nIndividuals and interactions over processes and tools\nI see the conflation of the ideas as a problem, because people are afraid to try something new, because it \u0026ldquo;isn\u0026rsquo;t Scrum\u0026rdquo;, or it \u0026ldquo;isn\u0026rsquo;t Lean\u0026rdquo; or worse, we don\u0026rsquo;t want to do a \u0026ldquo;hybrid\u0026rdquo;. Who cares! Just be agile. And let\u0026rsquo;s not forget the irony in not wanting to deviate from scrum when; the PO doesn\u0026rsquo;t sit with the team; the PO isn\u0026rsquo;t full time; there\u0026rsquo;s no scrum master; the team isn\u0026rsquo;t self selecting \u0026hellip; need I go on?\nI believe the key is to simply embrace the agile mindset. Be courageous with your process, because if you follow the agile values and principals you can trust that the process will deliver a delightful experience for both your team and your customer.\nOne cautionary note: agile is based on the empirical model for process control. That is, frequent inspection and adaptation for processes that are imperfectly defined and generate unpredictable and unrepeatable outputs. If you are going to change your process, ensure you have the means to measure the resulting change in some meaningful way, that will allow you to determine if it was beneficial or not.\nLastly, I wanted to quote a slide from Dave Thomas\u0026rsquo; Agile is Dead presentation (which is great if you haven\u0026rsquo;t seen it). He boils the agile process down to the following:\nFind out where you are Take a small step towards your goal Adjust your understanding based on what you learned Repeat When faced with two or more alternatives that deliver roughly the same value, take the path that makes future change easier.\nThat\u0026rsquo;s it! It\u0026rsquo;s as simple as developing a complex software system in a difficult business environment :) Hang tough though, have the courage to stick to the agile principals and let\u0026rsquo;s all make software development a better place to live.\n","date":"14 March 2017","permalink":"/posts/zombie-scrum/","section":"","summary":"One problem I\u0026rsquo;ve seen on several occasions now is that of Zombie Scrum, or Scrum-by-numbers.","title":"Zombie Scrum and the agile mindset"},{"content":"","date":"15 September 2016","permalink":"/tags/cli/","section":"Tags","summary":"","title":"cli"},{"content":"","date":"15 September 2016","permalink":"/tags/macos/","section":"Tags","summary":"","title":"macOS"},{"content":"I\u0026rsquo;m in love with VS Code, I\u0026rsquo;ll admit it. To make the joy complete, I wanted to fire it up from the command line (iTerm 2 of course).\nA quick Google resulted in this SO answer.\nThe TL;DR version - VS Code has a built in Shell Command to add itself to the path!\nPop open the command palette (Cmd + Shift + p) and start typing shell command, this will bring up some options, like so:\nJust select the install option, restart your terminal (or re-source your profile) and away you go!\n","date":"15 September 2016","permalink":"/posts/running-vscode-from-the-command-line/","section":"","summary":"I\u0026rsquo;m in love with VS Code, I\u0026rsquo;ll admit it.","title":"Running VS Code From the Command Line in Mac OS X"},{"content":"","date":"15 September 2016","permalink":"/tags/vscode/","section":"Tags","summary":"","title":"vscode"},{"content":"","date":"14 July 2016","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"14 July 2016","permalink":"/categories/development/","section":"Categories","summary":"","title":"development"},{"content":"","date":"14 July 2016","permalink":"/tags/webapi/","section":"Tags","summary":"","title":"webapi"},{"content":"I recently discovered that it\u0026rsquo;s possible to include a wildcard (the * character) in a WebApi route. It looks something like:\n[Route(some/route/{*key})] This ASP.Net article has a good example (search for the \u0026lsquo;wildcard\u0026rsquo; section).\nEssentially, everything beyond the route/ part of the url will be assigned to the key variable. For example, the route\nsome/route/more/parts/here?param=1\nresults in\nkey = \u0026#34;more/parts/here?param=1\u0026#34;; This can be useful for things like file storage and other uri params that may include folder paths or similar.\n","date":"14 July 2016","permalink":"/posts/webpi-route-wildcard/","section":"","summary":"I recently discovered that it\u0026rsquo;s possible to include a wildcard (the * character) in a WebApi route.","title":"WebAPI Wildcard In Route"},{"content":"When performing an HTTP Delete with a payload, WebAPI needs the content type to be specified, otherwise you\u0026rsquo;re likely to see a 415 - Unsupported Media Type response.\nHere\u0026rsquo;s a super simple delete request, with content-type included, for a JSON payload. Obviously, adjust the type to whatever your specific scenario requires.\n$http({ method: \u0026#39;DELETE\u0026#39;, url: links.blah.href, headers: { \u0026#39;Accept\u0026#39;: \u0026#39;application/vnd.hal+json\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, // \u0026lt;--- content type data: { dataNeededForDelete: someData } }); ","date":"14 July 2016","permalink":"/posts/http-delete-returning-415-unsupported-media-type/","section":"","summary":"When performing an HTTP Delete with a payload, WebAPI needs the content type to be specified, otherwise you\u0026rsquo;re likely to see a 415 - Unsupported Media Type response.","title":"Http Delete Returning 415 Unsupported Media Type"},{"content":"","date":"14 July 2016","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":"I recently had an odd experience where Git was, seemingly randomly, changing the capitalisation of my branch names. This would happen when performing operations like changing branches and pushing branches.\ntl;dr version, git stores branch prefixes like Feature/ in a folder and file systems aren\u0026rsquo;t case-sensitive. You need to delete the offending folder from .git/refs/heads. For a more detailed explanation, read on!\nIt\u0026rsquo;ll all started when I began using a branch naming scheme like Feature/branch-name and I thought that the Feature prefix may have been the issue \u0026hellip; and it kind of was, just not how I expected :)\nAfter some googl\u0026rsquo;ing around I discovered that Git stores branches as files and more importantly, it treats branch names like Foo/blah as a folder Foo within which there is a file blah.\n\u0026hellip; and file systems are not case-sensitive \u0026hellip;\nBooh-yay!\nTurns out I had accidentally created a branch name feature/something at some time, and even though I had deleted the branch name, the folder lived on. Thus every time I created a new branch with the Feature/ prefix, it would place it in the already existing feature (lower case) folder. While I was on the new branch, the prompt would be showing the uppercase Feature, but when changing away from, and then back to, the branch it would see the lower case folder. Thus explaining the strange case change!\nThe fix is quite simple. Given a branch prefix feature/ that you want to change to Feature, go to your Git repo, navigate to the\n../repo/.git/refs/heads\nfolder and delete the feature folder (making sure it\u0026rsquo;s empty of course).\nThe next time you create a branch Feature/foo there will be a new folder created with the correct capitalisation!\n","date":"14 July 2016","permalink":"/posts/git-branch-name-captitalisation/","section":"","summary":"I recently had an odd experience where Git was, seemingly randomly, changing the capitalisation of my branch names.","title":"Git Branch Name Capitalisation"},{"content":"Every once in a while I like to clean up my local Git repo and remove all of the local branches that have been merged with a branch on the remote. Rather than doing this one branch at a time, here\u0026rsquo;s a PowerShell one liner to do it for you (credit to this SO answer).\ngit branch --merged | ?{-not ($_ -like \u0026#34;*master\u0026#34;)} | %{git branch -d $_.trim()} It\u0026rsquo;s relatively safe, as it will explicitly ignore master and uses the little-d delete and --merged options.\nFor the curious, that SO answer also contains a bash variant for those in *nix consoles.\n","date":"14 July 2016","permalink":"/posts/git-delete-local-merged-branches/","section":"","summary":"Every once in a while I like to clean up my local Git repo and remove all of the local branches that have been merged with a branch on the remote.","title":"Git Delete Local Merged Branches"},{"content":"Although I\u0026rsquo;m a big fan of Git Extensions on Windows, I still prefer the relative safety of the command line when doing things like pushing to the remote.\nNow normally, using say PoshGit, I would type out something like\ngit push -u origin then cut/paste the branch name from the command prompt. Yes, I know that tab completion can be sort of a friend here, but often it\u0026rsquo;s a little slow and the cut/paste can still be quicker. Either way, it\u0026rsquo;s not a bad workflow.\nBut then I discovered that HEAD, as an alias to the current branch, works just as well as the branch name!\nSo just typing\ngit push -u origin HEAD works exactly as you\u0026rsquo;d expect - creating a remote branch with the current branch name!\nOf course you can use it to do a normal push, fetch etc, wherever the current branch name is needed.\nIt doesn\u0026rsquo;t help if you have a sudden case of push-to-master-itis, but still \u0026hellip; I think life changing isn\u0026rsquo;t too much of a stretch for this one :)\n","date":"14 July 2016","permalink":"/posts/git-push-the-easy-way/","section":"","summary":"Although I\u0026rsquo;m a big fan of Git Extensions on Windows, I still prefer the relative safety of the command line when doing things like pushing to the remote.","title":"Git Push The Easy Way"},{"content":"","date":"10 March 2016","permalink":"/tags/apidays/","section":"Tags","summary":"","title":"APIDays"},{"content":"My notes and highlights from APIDays 2016 in Melbourne.\nSlides are available for most presentations and are linked to from the section headings. You can also get to the slides via the abstract pages for each talk, which you can get to by clicking through the links to each individual talk in the APIDays program.\nSteve Sammartino - Welcome to Startup Land # This was the first keynote on day one and kicked things off with some interesting thinking around innovation, what it is, what it means and how it can challenge older / larger organisations who have operated in the same way for a long time.\nHe discussed how companies, in the context of innovation, should embrace horizontal thinking. The example being that of Fosters, a large brewing company and that they should be investing heavily in auto-driving cars :) Pretty funny, but also pretty savvy and set the tone for a number of examples of how innovation can come about through the connection of devices we never would have considered.\nAnother example was that of the connected toilet! Imagine a toilet that actually took your blood pressure, blood glucose levels, heart rate, dietary contents (ewww) and so on and fed that to your doctor! It\u0026rsquo;s a pretty brutal invasion of privacy (my first thought) but if it means that your doctor can catch cancer and tell you before you even notice any symptoms then that might just be worth it? \u0026hellip;\nOn APIs themselves, he stressed that you shouldn\u0026rsquo;t be afraid of selling an unfinished product, as this can allow you to develop an eco-system with some early consumers who in turn can help drive the product in new and unexpected ways. That is, let the consumers tell you what they want. Not so revolutionary but worth remembering!\nHe also made the point, again worth remembering, that how something works is less important than what it can do, particularly in the context of innovation.\nJames Bligh (NAB) - Digital Innovation in a 150 year old bank # The second keynote on day 1 and who knew NAB was so, well \u0026hellip; cool?\nHe started off with a list of points on why they felt the need to innovate, which seems to sum up the problem for most modern, large sized companies: consolidation (to support cross selling), proliferation (mobile, tablet, web), and competition (in this case traditional banking, as well as fintech).\nHe stated that tension drives creativity, which is actually kinda interesting (but clearly depends on the source of the tension, I imagine!)\nHe also aid that, for NAB, an API is the most cost effective and flexible strategy for reaching and reacting to their customers. Cool.\nThey also discovered a lot of unknown (or unexpected) usage, and had to adapt quickly, not just to maintain their service but to take advantage of opportunities as they arise.\nTheir architecture is actually pretty neat. They still maintain their core banking system (from probably decades ago) and enterprise service bus, with 18 month (yikes!) releases, which all must conform to the various government regulations. But, sitting on top of that is a bunch of service engines (read micro services) that give them far more agile capabilities for web and mobile applications. This provides them a way to integrate the different modes of change and different stakeholders at different levels.\nObviously, this was a massive undertaking and has been ongoing for some time (5+ years?). However, they approached it from a small-steps perspective (what, no more big-bang, $100mil projects? How surprising \u0026hellip;) and managed to define boundaries within the organisation to begin segregating services, which could then be built out one at a time into the various service engines. It would be interesting to find out how they manage to separate those pieces, which are often tightly and often surprisingly coupled. (He mentioned strict governance, which would help if there is a strong hand in what\u0026rsquo;s in and what\u0026rsquo;s out.)\nOne last thing James said, which I quite liked, was the following (and I\u0026rsquo;m paraphrasing a little):\nWhen faced with a choice, which one is more awesome? Which one are you/ would you be proud of?\nSeems like a pretty good way to make a decision :)\nKirsten Hunter (Akamai) - Irresistible APIs # This was another interesting presentation, focused on what makes an API great? What makes it something consumers want to use?\nNot surprisingly, her advice was to focus on the business value, not the technology. But she presented it from the perspective of telling a story and focusing on what does your API enable? Why is it useful?\nShe also touched on how to measure the success of an API? And it\u0026rsquo;s not just API key count :) You should be asking how are your users actually using your API? How active are they? Are they returning? Are they using it the way you want/intended?\nAnd most of all, are you capturing this data? (Hint: you should be, even if you\u0026rsquo;re not actively using / mining it right now, you may well be tomorrow).\nSome guidance she offered included don\u0026rsquo;t surprise your users and REST is not always best. Focus on usability and your driving use cases, not REST perfection. Also focus on deliberate design - she stressed that governance was important in ensuring that an API stayed focused and coherent.\nComing back to governance, she echoed one of the themes of the conference, which was on designing your API schemas in some modelling language (raml, swagger, blueprint, dsl?). The schema then becomes an artefact for use with developers, stakeholders and the exec level. Stay focused on the why.\nInterestingly, Akamai has an API working group enforcing schema model before API release, which acts as a contract. Too heavyweight? Don\u0026rsquo;t know, but seems like it could be beneficial in a larger organisation when there are competing forces. Of course, design by committee isn\u0026rsquo;t usually my first choice :)\nMichael Hyatt (MuleSoft) - User delight driven design of APIs # Live demo of RAML (RESTful API Modelling Language) to create an API schema in AnyPoint (from MuleSoft). Mentioned APX (API experience design). Is that a thing now!?\nOne interesting thing about RAML is that it includes the concept of traits (such as marking a collection as having an order).\nRob Zazueta (TIBCO Mashery) - The RESTed NARWHAL # This presentation focused on the NARWHL design framework for thinking about how to design adaptable API\u0026rsquo;s.\nNARWHL stands for Noun as A Resource With HyperLinks and is a resource oriented architecture (as opposed to service oriented).\nFrom the website:\nWhere REST is an architectural style for APIs, NARWHL is a framework intended to provide a roadmap for those needing to implement an API using current best practices but flexible enough to grow into the future.\nThey have also defined their own NARWHL Maturity Model similar to the RMM.\nAlso briefly mentioned Hypermedia (as everyone did), describing it as something that you describe, not define.\nOh, and categorically stated that client SDKs suck \u0026hellip; but are completely necessary right now. Ouch.\nShelby Switzer (Healthify) - APIs, Spreadsheets and Drinking Fountains # This one was kinda cool from a community perspective. She talked about how local communities are accessing government (and other) data and using it for the betterment of their local communities (be it avoiding parking fines to helping people find free parking spaces).\nShe also has an interesting GitHub project up called API in a box. From the README:\nAPI-in-a-Box is exactly what it sounds like. Say you have a handful of CSV files that you need a searchable API for. Put those files in Github repository, spin up this API-in-a-Box, and there you go! A REST hypermedia API that utilizes Elasticsearch\u0026rsquo;s killer searching.\nIt was interesting to see the unexpected uses of open data. If data is not made available, then these connections will never be made, no matter how much innovating government (or other) tries to do.\nGraham Lea (Tyro) - Building a bank out of micro services # These guys where a payment gateway, but their customers where hurting due to the banks delays in moving money from one account to another. So they decided to become a bank! Nice. They also did it to take advantage of the flexibility and low time to delivery for MVP type features. Even nicer!!\nHe started off with a paraphrased take on Cockcrofts definition of a micro services architecture, describing it as a loosely coupled distributed architecture with bounded contexts.\nI believe the original is:\n\u0026hellip; a service-oriented architecture composed of loosely coupled elements that have bounded contexts.\nI like the subtle change to distributed architecture, but he also had a whole list of guidance around building APIs. He recommended ensuring a single responsibility for each service, and that you must define and document your services. He talked about responsible services, which are services that use as few other services as possible.\nHe also touched on data ownership, and that it\u0026rsquo;s a good idea to have one schema per service, independent of all other services. In this way you can avoid inappropriate joining (why does that sound so funny? \u0026hellip; ahem)\nMore tips included caching data where processing happens, preferring async communication wherever possible and that if two services are very chatty then they are probably the same service (Hint: combine them into one, or reconsider their contexts, because their boundaries are not well defined or separated). They also used Pact for testing.\nOne more general piece of advice he gave was to organise the team around the work. This is really good to remember, as organising the work around the team is just silo\u0026rsquo;ing and shifts focus away from features / business value.\nSteven Willmott (3scale) - Entering the platform age # The keynote from the second day, and stressed again the need for governance and communication around your APIs. He also made an interesting point about the shift from 1-1 to 1-N client server interactions, and even further to N-N interactions with the proliferation of both connected devices and services.\nHe made a great point that the value of any platform lies in allowing co-creation between providers and consumers. Makes sense.\nHe also put up a diagram with expectation on one axis (ranging from expected to unexpected) and outcome on the other (ranging from desirable to undesirable). He then focused on the undesirable outcomes and made the wonderful connection of innovation as something that is unexpected and desirable. He also described the alternative of unexpected and undesirable as a fire drill :)\nOther interesting guidance was to not focus on the developer, as although they may be the enabler of the user, it is actually the user who really consumes your API (Hint: focus on the why of your API, not the how). Logically following on from this he noted that the measure of success of an API is not the number of hits, but the number of businesses (or amount of business value) enabled.\nOne last bit of advice was that voice activation / commands will change APIs because they will need better ordering and filtering, it\u0026rsquo;s simply not good enough to return just pages of data. Not sure I have an opinion, but it is interesting to consider what happens if the consumers usage changes subtly (typing in a search box becomes asking Siri to find a nearby restaurant), and what ramifications that might have for your APIs usability.\nUli Holtel (BankWest) - following the signs # This one was about hypermedia, which he uniquely described as runtime intellisense for REST APIs!\nHe did have some other guidance, based around what level of coupling your API and client can successfully support. Some scenarios to keep in mind are: Internal versus external; API ownership; Single versus multi-client; Process ownership (are you the thought leader of the process?); Rate of change; and API context sensitivity (how sensitive is your API usage to web client, mobile device, tablet etc).\nHe also mentioned using Postman as a tool for exploring APIs. I haven\u0026rsquo;t used this before, but looks quite nice.\nTim liddlelow (ANZ) - an API primer # No slides link available at the time of writing.\nThis was kind of a surprise. He had some really thoughtful stuff to say on designing your APIs, and that APIs are about exposing your business data to a consumer not an application programming interface in the traditional software library sense.\nThe quote of the conference for me (paraphrasing a little):\nAPIs are 20% tech and 80% people/ business.\nHe also noted that API management is very different to desktop applications or even web applications management, but it\u0026rsquo;s crucial to monitor your API and track usage analytics.\nBrett Adam - API or die trying # Some really useful insights into building APIs. Again he reinforced the need to gather analytics for your API. Things like usage, context and work flows. You should also keep in mind that you need to correlate usage with accounts as people will use your API in ways you never imagined and you want to see that data.\nHe mentioned the Charles tool for watching HTTP traffic and noted that you really need to be aware of what can be sniffed from your API (obviously this will be more or less serious depending on the data you\u0026rsquo;re sending).\nQuote of the presentation:\nUse HTTP / REST and get over it.\nHe talked a bit about rate limiting and that it can be a good idea to include a \u0026ldquo;retry after\u0026rdquo; meta data element in HTTP header.\nHe also noted that side-loading is OK. Be pragmatic about it and pre-package data if necessary. You don\u0026rsquo;t have to deliver everything over your API. And get your versioning sorted early!\nHe posed the question: Why is REST the only real API?\nAnswer: The enormous amount of machinery between the user and back end (proxy, firewall, servers) and that you\u0026rsquo;re likely to be hitting a different server every request when at scale. The point being these things only work well with REST, building on all the intelligence (caching etc) along the way.\nAdeel Ali (Apimatic) - Apis in the real world # No slides link available at the time of writing.\nThese guys analysed around 11,500 APIs! Some of the findings where that 70% didn\u0026rsquo;t include an authentication type in a discoverable way and that only 12% included category information. Honestly I\u0026rsquo;m not sure of the significance of either of those, but it does speak to the larger issue of discoverability of an API. If you\u0026rsquo;re going to make a public API then make it discoverable! Or don\u0026rsquo;t make it public. Geez.\nHe also stressed (probably because they\u0026rsquo;ve tried to build an automated API discovery tool) that you really shouldn\u0026rsquo;t violate the basic tenants of HTTP (safety, cachability, idempotency), but otherwise do what best suits your need.\nNick Ward (MS) \u0026amp; Jorge Arteiro (Kloud) - Mobile innovation in the mobile first, cloud first world # Microsoft have consolidated their APIs! Try it out here.\nIt\u0026rsquo;s kinda awesome sauce (any parents of Furby toting toddlers out there?)\nAnd MS has a complete API reference here aka.ms/apiref.\nOh, and then there\u0026rsquo;s Project Oxford - a bunch of AI based APIs for speech, vision and language. Super awesome sauce ;)\nRob Valk (Sixtree) - Scaling the bikeshed with Jason API # No slides link available at the time of writing.\nProvided some nice thoughts on how to think about your API and that\u0026rsquo;s it\u0026rsquo;s a resource graph, not domain graph. Trotted out HATEOS (Hypertext As The Engine Of State) again, which is the Worst Acronym Ever (WAE?).\nHe talked about bikeshedding, a term I hadn\u0026rsquo;t heard before, but means spending too much time on trivial issues. It comes from Parkinsons Law of Triviality - go read the Examples section (it\u0026rsquo;s pretty funny), but it revolves around a local council debating the colour of a bike shed that was to be built next to a nuclear reactor.\nHe also mentioned JSON API and that it\u0026rsquo;s best to use a library (and there are plenty available). One interesting feature is that you can use the include property to force linking to duplicated entities (and this is why you want to use a library to automate the construction of the JSON!)\nJSON API also plays much nicer with dynamic languages, but less so with static languages where the effort to parse the structured JSON back into objects is onerous (hence (again) the advice to go use a library).\nSummary # Overall quite an interesting and diverse bunch of presentations. I liked the repeated focus on designing your APIs well, in a cohesive and discoverable way.\nAPI management also came up a bit, although no one really offered much in the way of a solution or guidance, other than it\u0026rsquo;s difficult and requires attention.\nDesign first was also a major topic, but again there are lots of options available here. Just keep focused on why you are building your API.\nI also hadn\u0026rsquo;t thought to much about APIs and being context aware, but I guess this is really something that will only become more and more salient with the proliferation of connected devices (mobiles, tablets, browsers, toilets, fridges, TVs \u0026hellip;)\nLastly, there where a number of people who described a split in their development teams, almost always around the API \u0026ldquo;layer\u0026rdquo;. I think this is OK, so long as people are not pigeon holed on either side and everyone is involved in the API design.\n","date":"10 March 2016","permalink":"/posts/apidays-2016-highlights/","section":"","summary":"My notes and highlights from APIDays 2016 in Melbourne.","title":"APIDays 2016 Highlights"},{"content":"","date":"10 March 2016","permalink":"/tags/conference/","section":"Tags","summary":"","title":"conference"},{"content":"","date":"3 March 2016","permalink":"/tags/yow/","section":"Tags","summary":"","title":"YOW"},{"content":"Some highlights and a short summary from my attendance at YOW! 2015. All the presentations are online at the YOW! Australia YouTube channel.\nNote that the section headings are linked to the corresponding presentation video. I attended the Melbourne conference, but some of the videos are from Sydney or Brisbane events.\nDon Reinertsen - Thriving in a Stochastic World # Keynote Day 1 Why deliver the plan? # Don\u0026rsquo;t focus on the plan - the plan has no intrinsic business value. The plan is a way to describe a sequence of steps to reach the business goal and provides a starting point. Of course, as agile software developers, we know to stop and inspect at small increments and adjust as necessary. The plan is a live artefact, constantly evolving with the software.\nWhen we change the plan, we are making the assumption that the work we do has more value than the work we don\u0026rsquo;t.\nVariability (a good thing) # Focusing on the plan can reduce variability. But, this reduced variability will in fact reduce the possible pay-off significantly. This is because the delay-to-cost ratio is too high when fixated on the plan. The delay-to-cost ratio simply defines the cost of delaying doing something, versus the cost of doing it now. From an economic stand-point, this is modelled as an asymmetric pay-off function. Without variability the eventual pay-off is significantly reduced.\nDan North - Delivery Mapping # Probably my favourite talk, along side Dave Thomas\u0026rsquo; (pragdave) \u0026ldquo;Agile is Dead\u0026rdquo; talk.\nA couple of cracking quotes including describing the current state of \u0026ldquo;Agile\u0026rdquo; software coaching and mentoring as a our own \u0026ldquo;ponzi certification scheme\u0026rdquo;. Love it!\nHad a really simple proxy for using the Dreyfus mode of skill acquisition, which is working for him:\nNone (never seen it, wouldn\u0026rsquo;t know what it looked like) Read (could read it, maybe intuit the intention) Write (could modify and adapt it) Teach (could teach it) Intuitively, as a developer, this seems like a really nice way to measure skill. We use a 1 - 5 measure here at Readify, but I think this might be better. No good evidence for that tho \u0026hellip;\nThe core of the talk was on Delivery Mapping, which I hadn\u0026rsquo;t seen before. Really interesting stuff, especially combing the skills matrix with the project and looking for gaps as well as opportunities for development and exploration. In particular, looking for opportunities where you have skills but no business need, but business need with no skills - slam em\u0026rsquo; together and get a Cobol re-write in Python that\u0026rsquo;s faster and more maintainable. Not to mention the engagement this would bring for the employees involved. That\u0026rsquo;s a cool idea.\nHe also discussed Risk Adjusted Return on Capitol (RAROC). Another idea from finance that helps us software nerds understand why iterative development works for the business. By just considering sunk costs versus profit, you get an inverted triangle, with a long time before seeing a ROI. In the iterative and incremental world, we begin getting payback much sooner. That\u0026rsquo;s great and we\u0026rsquo;ve known it for a while. But, you combine this with the asymmetric payoff functions from the keynote and you get a really complete and compelling story as to why this software agility works for a business. Embrace change! You\u0026rsquo;ll be rewarded with significant gains in pay-off.\nDave Thomas (pragdave) - Agile is Dead # I\u0026rsquo;ve seen this talk before (YouTube), but he\u0026rsquo;s certainly softened the intro a little. Still, it\u0026rsquo;s a tremendous talk and one designed to challenge us as agile (yes, the adjective) software developers. This talk was right after Dan North, making this session the major highlight of the two days (at least for me).\nIn essence, he believes that the noun\u0026rsquo;ification of Agile has single handedly led to some really bad stuff in our industry (ponzi certification scheme\u0026rsquo;s anyone?) as well as SAFe \u0026hellip; really? WTF!? How did we get here \u0026hellip;\nHe boils it down to the following (I\u0026rsquo;m paraphrasing here and using my own language, but the concept is the same) - as an agile software developer, who believes strongly in the Manifesto for Agile Software Development, we have three core things to do:\nInspect - where are we? What\u0026rsquo;s our current context? Adapt - where do we want to go? And (importantly) what\u0026rsquo;s a small step to get there? Make the step, rinse and repeat. Lastly, if faced with a choice of two things with equal value, choose the one easiest to change in the future.\nGlen Vandenbourg - The Future of Software Engineering # This was a surprise highlight for me. I didn\u0026rsquo;t know what to expect, but he made some really great points regarding the state of our industry, how we really are an engineering discipline, and how we originally made the mistake of picking the engineering discipline (Civil) that is the least like software development.\nAs an aside - did anyone else not know that NATO (yes, that NATO) called a world wide conference on the crisis in Software Engineering in the 50\u0026rsquo;s!? I\u0026rsquo;m not saying we have a crisis in our industry now, but we really don\u0026rsquo;t like looking back, do we?\nMany engineering disciplines are based heavily on the empirical process, because they operate in systems of great variability. Industrial and chemical are two such disciplines. His main point was that this is essentially the same as software development. We work in a stochastic (another keynote!) environment, with high variability. Let\u0026rsquo;s embrace it - interestingly, Scrum is based on empirical process control. Inspect and adapt!\nOne other interesting point for me was how we (as an industry) often try to map traditional (Civil) engineering roles to software:\nArchitect/Designers = Software Architect / Business Analysts etc Plans = Model (UML / ?) Labourers = Developers Building = Application But in his view, a much better mapping is:\nArchitect/Designers = Development Team (Scrum language - everyone involved in building the product) Plans = Code base + artefacts Labourers / Builders = compilers and tooling Building = Application The major (major) difference being that the most significant cost in the first scenario is the labour cost to actually build the thing. In the second scenario, the major cost is the architect and design phase and the actual build is practically free (a couple of seconds/minutes to compile).\nHe then noted that all the process optimisations from Civil and the like are focused around the cost to build. Naturally, this is a terrible fit for software. Thus we need to look at the more empirical engineering disciplines as a better starting point for software engineering.\nI\u0026rsquo;m not sold on the idea of software being a true engineering discipline, but I can\u0026rsquo;t help but agree that some rigour needs to be around the processes we use, otherwise nothing is repeatable or measurable. Without being able to measure our state (inspection) we\u0026rsquo;ll never be able to move forward with any certainty and when we are successful, we won\u0026rsquo;t know why and thus we\u0026rsquo;ll never bring any repeatability to our process.\nInteresting stuff and worth keeping an eye on.\nUber - just plain broken \u0026hellip; # Wow, a super high energy talk! The guy from Uber just took-off and never looked back. And man, Uber is broken! But I guess it works for them :) Really interesting to see how they work (or perhaps don\u0026rsquo;t work) and how they nearly lost everything a couple of times.\nI still have no idea how he managed to go from Uber, to Master / slave relationships, to Nazi Germany \u0026hellip; too much RedBull perhaps?\nNote that the video link is to the Brisbane event. I couldn\u0026rsquo;t find the Melbourne one, possibly because of the reference above!? Fascinating stuff though.\nAino Vonge Corry - A Comment on Learning # Another surprise highlight was a presentation on different learning types. While its intuitively obvious that people will learn in different ways, it was quite interesting to hear about the specific ways in which learning occurs.\nThe presenter broke it down into a persons preference for learning (active or reflective) and a persons preference for detail (global or specific). In essence, people who are more active learners will prefer to ask questions and engage with the instructor. A reflective learner will want to go away and take time to think about the material before asking questions.\nSimilarly, a person who prefers global information will want the big picture details, a high-level view providing a wide context. They want to know the why of it. A person who prefers specific detail will want specific examples and much higher detail - they want to know the how of it.\nObviously, everyone is different and most people will not be on the extremes of those preferences. And they are just preferences, we can learn to use a non-preferred learning technique. But it is useful to keep in mind when designing a presentation that some in your audience may want big-picture detail and others more specifics and that you can\u0026rsquo;t please everyone with every slide (unless it\u0026rsquo;s a funny cat picture. Everyone loves them.)\nSummary # Looking back over this, I kinda feel like I\u0026rsquo;m \u0026ldquo;Agile\u0026rdquo; bashing a little. Personally, I\u0026rsquo;ve never been big on certifications. About the only one I do have is the Profession Scrum Master (PSM I) certification from Scrum.org. I loved the PSM course and it taught me a lot. But the thing I got most out of this conference was that, at the end of the day, it\u0026rsquo;s the Manifesto of Agile Software Development values that are most important to me. I must always remember to continually inspect and adapt to better understand the current context and make better choices for those small steps, in order to best serve the client and the client\u0026rsquo;s business.\nNot having much (ok, any) of a financial background, I also really liked the basic economic models that support the concepts of incremental delivery. Really handy to have in the back pocket when discussing process change, to give a complete story of the benefits of agile software development (especially when the topics of risk and variability come up!)\n","date":"3 March 2016","permalink":"/posts/yow-2015-highlights/","section":"","summary":"Some highlights and a short summary from my attendance at YOW!","title":"YOW! 2015 Highlights"},{"content":"","date":"22 November 2015","permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure"},{"content":"Recently, Mohamed (a fellow Readifarian) and I spent some time creating a complete build and deployment pipeline. Our main goal was to create a fully functional CI server in Azure, with TeamCity for builds and OctopusDeploy for deployments.\nWhy? Firstly, although we both had some experience in parts of the technologies involved, neither of us had built a complete end-to-end environment, from scratch. Just sounded like too much fun!\nSecondly, during my career as a software developer one of the things I\u0026rsquo;ve noticed is that many sites do not have a fully automated build and deployment environment. This just makes life incredible difficult, for a large number of reasons, including:\nbuilds are not repeatable, merging long lived feature branches becomes a nightmare, creating a release is often a manual and error prone exercise, and deployment out to production is a cross-your-fingers, roller-coaster ride of disappointment. This post includes:\nthe key learnings we had while setting up the environment, and links to some more detailed information we found useful. This post won\u0026rsquo;t include:\ndetailed step-by-step instructions on setting up Azure, TeamCity and OctopusDeploy (there are plenty of those, see below), or automation with PowerShell (that\u0026rsquo;s for another post!). tl;dr # The executive level summary is to go read this post by Andy French. It\u0026rsquo;s what we used as a guide and it provides great step-by-step instructions for getting a build server up and running (including Azure VM and TeamCity).\nFrom there you just need to get Octopus installed and you\u0026rsquo;re away!\nOf course, there were a number of little (and not so little) issues we encountered along the way, which the rest of this post will cover.\nCreating the Azure VM # Creating a VM on Azure is remarkably simple. The basic order of things is to:\nCreate a Virtual Network Create a Storage Account Create a SQL Server DB Create a Cloud Service Create a Virtual Machine Step 1 - Virtual Network # For our needs, setting up a Virtual Network was overkill, thus we skipped this step. However, if you would like to setup a multi-machine environment on the one sub-net in Azure, then you will need to do this. For us the stand-alone VM option was perfectly suitable.\nStep 2,3 - Storage and Database # Steps 2 and 3 are optional. A Storage Account will be automatically created for you when you create the VM if you don\u0026rsquo;t already have one defined.\nAs we\u0026rsquo;re using TeamCity, we chose to create a Azure SQL Server DB. TeamCity has it\u0026rsquo;s own, built-in database (HSQLDB), which you could use as a trial, but it\u0026rsquo;s not for anything else. From the TeamCity documentation:\nThe internal database may crash and lose all your data (e.g. on out of disk space condition). Also, internal database can become extremely slow on large data sets (say, database storage files over 200Mb). Please also note that our support does not cover any performance or database data loss issues if you are using the internal database.\nIn short, do not EVER use internal HSQLDB database for production TeamCity instances.\nNote that the emphasis above is from the TeamCity doco, not mine.\nLastly, if you\u0026rsquo;re going to be using TeamCity for non-Windows builds, it\u0026rsquo;s strongly recommended to select a collation type ending in \u0026lsquo;CS_AS\u0026rsquo;. From the TeamCity SQL Server documentation:\nAs the primary collation, it is recommended to use the collation corresponding to your locale. We also suggest using the case-sensitive collation (collation name ending with \u0026lsquo;CS_AS\u0026rsquo;), which is mandatory for the certain functionality (like using non-Windows build agents).\nAnd here\u0026rsquo;s a screen capture of that setting in the classic portal:\nAnd here\u0026rsquo;s what the setting looks like in the preview portal:\nNote We created a SQL Server database for TeamCity, but it can use most of the popular databases. Going the SQL Server route was simplest in this instance, however jump into the TeamCity docs to get a MySql or PostgreSQL installation rocking if that suits your needs. Step 4 - Cloud Service # A Cloud Service is a (Azure processing unit) container for one or more virtual machines, giving you the ability to load balance your service. The image below is from the linked Cloud Services article above and gives a brief description of the different hosting models for you application:\nIn this instance, we wanted to create our own VM, but we did not need the added features (and complexity) of the Cloud Services model. Azure makes this easy, as when you create the VM you can either:\nselect an existing Cloud Service, or one will be automatically created for you. We took the latter option.\nAlso note that Azure will automatically name the Cloud Service with the same name as the VM, which means they will have the same DNS name (eg, the service and the VM will both be at myvm.cloudapp.net). There were some articles on SO that indicated this may be a problem, but we never had an issue (except for when I forget to open the appropriate ports on the VM\u0026rsquo;s local firewall, but that\u0026rsquo;s another story \u0026hellip;). In more complex, multi-VM environments this may be something to consider.\nStep 5 - Creating the Virtual Machine # When creating a VM, you really just need to select:\nan image name, a name for the machine, the tier and size, and the region. If you\u0026rsquo;re going for a build server, as we were, when selecting the image name be sure to pick one with VisualStudio installed \u0026hellip; perhaps too much coffee that day. In the end we had success with both Windows Server 2012 and Windows 10 VM\u0026rsquo;s.\nThe easiest way to do this is to go to the Azure Marketplace:\nThen just enter \u0026lsquo;Visual Studio\u0026rsquo; in the filter at the top of the page. Note that you may only see entries for Visual Studio 2013 in the drop-down:\nJust hit Enter in filter box and wait for the full list to display. From there just pick the edition and release that suits your needs:\nDesktop Image or Server? Good question. My general thoughts here were that if your application is designed to run on a desktop OS, then build/deploy on a desktop OS. However, Microsoft\u0026rsquo;s licensing may have something to say about that. I imagine your mileage will vary with this one, but there is a large number of pre-defined images to choose from, all pre-installed with VisualStudio and/or the build tools. Also, when you first create a VM be sure to allocate some decent resources, at least multi-core and plenty of RAM. If you don\u0026rsquo;t, your RDP sessions will drive you mad with the lag. It won\u0026rsquo;t take long to configure (well, not if you follow this advice!) and you can down-grade the machine once your done.\nThere is also an option to select an Availability Set. I believe this allows you to configure fail-over into other regions / data centres, but for our purposes we just selected the default of \u0026rsquo;(None)\u0026rsquo;.\nIf you\u0026rsquo;re using the new Azure management portal, you will also see an option for the Deployment Model. This option is not available in the classic view and allows you to select:\nClassic (also known as Service Managed) and is the same as using the old portal, or Resource Managed this is a new option, allowing deployment of groups of resources as a single entity. Here\u0026rsquo;s what it looks like in the new portal:\nFollow the link to more detail on the differences between Resource and Service Managed deployment models. Note that they are not entirely compatible. From the article:\nThe Resource Manager deployment model provides a new way to deploy and manage the services that make up your application. This new model contains important differences from the classic deployment model, and the two models are not completely compatible with each other. To simplify the deployment and management of resources, Microsoft recommends that you use Resource Manager for new resources, and, if possible, re-deploy existing resources through Resource Manager.\nLastly, be sure to update the Endpoints configuration to add any port exceptions you may need (this was especially true for OctopusDeploy, which we put at the non-standard port 8080). This can be done when you create the VM as well as later on in the configuration settings.\nGotcha Don\u0026rsquo;t forget to add exceptions for the various ports to your VM Endpoints config, but you must also add them to the VM OS firewall as well. Yeah, that one didn\u0026rsquo;t bite me \u0026hellip;\nLearnings # A few other miscellaneous things we learnt, in no particular order:\nWe both started with the the new management portal (https://portal.azure.com), but switched back to the classic view (https://manage.windowsazure.com). In general, I found the new portal a little less friendly than the classic view. It also didn\u0026rsquo;t flow as nicely on my 13\u0026quot; MBP, but it might be a more pleasurable experience on a larger monitor. Be careful with your usernames and passwords. I got all \u0026lsquo;secure\u0026rsquo; early on and created different named accounts with random passwords for just about everything and it was a bit of a nightmare to manage. Be secure. Just be a little pragmatic and don\u0026rsquo;t go nuts. Save. Don\u0026rsquo;t forget to save in the Azure management portal. What was that? Oh, did you forget to save? Save. Save. Save. Conclusion # Hope that was useful. Coming up in Part 2 is TeamCity installation and setup.\n","date":"22 November 2015","permalink":"/posts/azure-teamcity-and-octopusdeploy-part-1/","section":"","summary":"Recently, Mohamed (a fellow Readifarian) and I spent some time creating a complete build and deployment pipeline.","title":"Azure, TeamCity and OctopusDeploy - Part 1"},{"content":"","date":"22 November 2015","permalink":"/tags/ci/","section":"Tags","summary":"","title":"CI"},{"content":"","date":"22 November 2015","permalink":"/tags/octopusdeploy/","section":"Tags","summary":"","title":"OctopusDeploy"},{"content":"","date":"22 November 2015","permalink":"/tags/teamcity/","section":"Tags","summary":"","title":"TeamCity"},{"content":"","date":"30 October 2015","permalink":"/tags/burnout/","section":"Tags","summary":"","title":"burnout"},{"content":"","date":"30 October 2015","permalink":"/tags/people/","section":"Tags","summary":"","title":"people"},{"content":" Photo by Rod Long on Unsplash Making the most of a difficult situation. Toxic Environments # As a developer I\u0026rsquo;ve seen some pretty bad stuff. Staying strong and continuing to enjoy doing the career path I\u0026rsquo;ve chosen for myself hasn\u0026rsquo;t always been easy. Sometimes you\u0026rsquo;re in a toxic environment, could be team mates, management, resources, time pressure or all of the above. You feel trapped and essentially powerless to affect change. Lots of us have been there, but what do you do?\nOne of the reasons I joined Readify was to steer my career down a path where I would be able to affect greater change, across multiple organisations, than I could doing plain old full-time development.\nBut sometimes gigs just suck.\nMaking change is hard. You can make all the noise you want, but I strongly believe that unless you are in a position to affect cultural change, there is little chance that the situation will improve. Sure you can add all the ceremonies you like, but the underlying suck-factor will still be high.\nThere are people who are brilliant at this, able to ride in on a white horse and give the tree a good shake. That\u0026rsquo;s fine if you can walk-away if it doesn\u0026rsquo;t work. Unfortunately, we live in a commercial reality and we get stuck in environments we would never choose.\nWhat\u0026rsquo;s a Toxic Environment? # I\u0026rsquo;ve seen and heard of a lot of bad environments. Environments where there are lots of problems, like:\nThe proxy server is slow / broken / miss-configured / laughing at us. IT just ignore us and we don\u0026rsquo;t have the resources we need. Management don\u0026rsquo;t get what we do and we don\u0026rsquo;t have enough people. The PO doesn\u0026rsquo;t have time for us. These are all bad situations, but not necessarily toxic (although they certainly can become so!).\nA toxic situation is one in which you, as a developer, are:\nfundamentally compromised in your ability to work effectively, do not feel safe to air your opinions or grievances, feel pressured to compromise your professional integrity. We must all be pragmatic from time to time. But when that is the norm and there is tremendous pressure to do the wrong thing repeatedly, that\u0026rsquo;s toxic.\nBelow are some of the things I\u0026rsquo;ve learnt that help me get through the times when I feel like I\u0026rsquo;m pushing that barrow up a hill. Hopefully some of it will be useful for others and I\u0026rsquo;m all ears when it comes other peoples strategies as well - sharing these ideas as a community is something we do well and can only serve to improve those less fortunate than ourselves!\nStep 1 - Listen and Learn (Inspect) # To quote Mickey Mouse, \u0026ldquo;we\u0026rsquo;ve got ears, say cheers!\u0026rdquo; Ears are great. It\u0026rsquo;s tempting to try and fix a bad situation quickly, but personally I find it easier to sit still and get myself engaged in the existing process, no matter how bad. It\u0026rsquo;s at that point that you can start to do the math and put together a more complete picture of what\u0026rsquo;s happening. Look at the people, look at the code base, look at the process.\nToxic environments are rarely what they initially seem and you need to really understand the root cause of issues before you will have any chance of making a difference. Ask yourself the five whys.\nStep 2 - Communication and Leadership (Inspect and Adapt) # Be open, honest and transparent at all times. Toxic environments generally have poor communication channels and words can be easily twisted. Make sure you are clear in your intentions and be specific in what you are doing and what you are trying to achieve. Keep a journal. This is just a good idea in general, but it can really help at the end of a bad day to dump all the negative crap onto a page, forget it and go home.\nThis can be a really good opportunity to show some leadership skills by doing the right thing. I\u0026rsquo;ve been in numerous places where it\u0026rsquo;s really hard to do the right thing, as after the 3rd time getting shot down by the PO in a meeting, you just give up and go with the flow. Set the example by doing the right thing even if it feels like a complete waste of time \u0026hellip;\nbecause \u0026hellip; others see you doing the right thing - builds trust, making people feel safe and thus more likely to open up and just sometimes you get some traction with others who will follow your lead.\nSafety is all important. People will generally be far more likely to open up about existing problems if they feel safe. Water cooler conversations are great. Impromptu one-on-ones after a stand-up are great.\nUnfortunately, in toxic environments, the only people talking are, in general, not the ones you need to be listening to. Build rapport with the team and not only will it help you understand the environment, but it\u0026rsquo;ll help stave off the feelings of isolation and make life just a little more bearable.\nStep 3 - Change (Adapt) # In the initial phase of building a relationship there is no expectation of behaviour. Over time, that changes and often once in the rut, it\u0026rsquo;s very difficult to get out of established patterns. Likewise, in established teams, the relationships are ingrained (this is really true in toxic environments).\nHowever, if you change your interaction with someone, they are forced to change there interaction with you. Don\u0026rsquo;t be afraid to get out of the rut and force some interaction change.\nRamp up the communication and (constructive!) feedback, as well as the frequency. Don\u0026rsquo;t let those on your team you know are suffering do it in silence. You don\u0026rsquo;t have to wait for R U OK day to talk to your team mates and colleagues!\nAnd if you\u0026rsquo;re suffering, try and find a sympathetic ear. Even having a vent to a colleague outside the organisation can help relieve the pressure.\nNegative thinking usually creeps in pretty quickly too. For example, it\u0026rsquo;s pretty easy to start thinking something like \u0026ldquo;He\u0026rsquo;s the problem\u0026rdquo;. Um, no. He\u0026rsquo;s not the problem. His management style might be a problem. Her approach to software development might be a problem. But in general, people are doing their best. Just because someone isn‚Äôt good at their job, and in toxic environments this is pretty common at the leadership (or lack of) levels, doesn\u0026rsquo;t mean they are intentionally trying to make your life hell. This is a great opportunity to take a step back and think about how you can help them be better at their job.\nAnd it\u0026rsquo;s never personal. Even if sometimes, the odd cray cray individual wants to make it so, it\u0026rsquo;s never about you. Stay calm and immediately reach out to your leadership group as this sort of stuff is completely out of order.\nSilver linings # Crappy jobs are a great place to look for some PD. Look for new technologies in use that you\u0026rsquo;d like to learn more about, practice you\u0026rsquo;re consulting skills, make friends with the local barista, find another cycling nut in the office. All of these things can make your life just a little bit easier.\nThe pressures in a toxic environment are different. Usual there\u0026rsquo;s less pressure to be technically brilliant and more pressure to push out code quickly, not lose your mind in yet another 4 hour meeting or scream at the marketing guy who just tripled the price of your product because he\u0026rsquo;s behind on his sales KPI and doesn\u0026rsquo;t want to lose his bonus. Again, stretch your non-developer skills and see if you can make even a small difference.\nIn Summary # Can you always affect change? Nope. Can you make a difference while you\u0026rsquo;re there? Quite possibly. It\u0026rsquo;s one of the main reasons I became a Readifarian. To quote another wise animation, \u0026ldquo;Just keep swimming\u0026rdquo;, it might feel like you\u0026rsquo;re swimming through mud, but if you keep doing your best, looking for opportunities for personal and professional PD, you\u0026rsquo;ll find that others will begin coming to you, even just to vent frustrations. At the very least you\u0026rsquo;re getting people to be more open about the problems and setting up the communication channels that (just might) lead to some real change.\n","date":"30 October 2015","permalink":"/posts/working-in-a-difficult-environment/","section":"","summary":"Photo by Rod Long on Unsplash Making the most of a difficult situation.","title":"Working in a Difficult Environment"},{"content":"In a previous post I talked about [terminal replacements for Mac OS X]({% post_url 2015-10-22-terminal-replacement-on-mac-os-x %}). Unfortunately, on Windows the choices are less numerous.\nPowerShell is pretty awesome and on Windows 8/10 it quickly became my default environment, especially with PosGit installed. However, I was still looking for something \u0026hellip; more nix\u0026lsquo;ish.\nI tried Babun, but I found the cygwin environment just foreign enough that I wasn\u0026rsquo;t comfortable with it.\nEnter Cmder. It‚Äôs based on ConEmu but adds some nice Git and other configuration (make sure you grab the msygit version). It also supports tasks, which allow you to quickly and easily create new tabs with, for example, an admin PowerShell, or a Git Bash terminal, or an admin chocolatey session.\nEnjoy!\n","date":"23 October 2015","permalink":"/posts/replacing-the-windows-console-with-cmder/","section":"","summary":"In a previous post I talked about [terminal replacements for Mac OS X]({% post_url 2015-10-22-terminal-replacement-on-mac-os-x %}).","title":"Replacing the Windows console with Cmder"},{"content":"","date":"23 October 2015","permalink":"/tags/terminal/","section":"Tags","summary":"","title":"terminal"},{"content":"","date":"23 October 2015","permalink":"/tags/windows/","section":"Tags","summary":"","title":"windows"}]